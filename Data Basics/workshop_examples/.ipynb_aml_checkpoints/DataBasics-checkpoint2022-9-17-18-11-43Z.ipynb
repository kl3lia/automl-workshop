{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#authentication\r\n",
        "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
        "from azureml.core import Workspace\r\n",
        "\r\n",
        "ia = InteractiveLoginAuthentication(tenant_id='16b3c013-d300-468d-ac64-7eda0820b6d3')\r\n",
        "\r\n",
        "# You can find tenant id under azure active directory->properties\r\n",
        "ws = Workspace.get(name='Prod',\r\n",
        "                     subscription_id='fe38c376-b42a-4741-9e7c-f5d7c31e5873',\r\n",
        "                     resource_group='ProdRG',auth=ia)"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1666026297467
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ** Create Tabular Dataset from WEB URL (CSV files) and Use Unregistered Dataset Directly in Training Script**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creates an unregistered TabularDataset from a WEB URL.\r\n",
        "\r\n",
        "from azureml.core.dataset import Dataset\r\n",
        "\r\n",
        "web_path ='https://dprepdata.blob.core.windows.net/demo/Titanic.csv'\r\n",
        "titanic_ds = Dataset.Tabular.from_delimited_files(path=web_path)\r\n",
        "titanic_ds.take(3).to_pandas_dataframe()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 30,
          "data": {
            "text/plain": "   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500  None        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250  None        S  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>None</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>None</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1666026345514
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set data schema\r\n",
        "#when you create a TabularDataset, column data types are inferred automatically. \r\n",
        "#If the inferred types don't match your expectations, you can update your dataset schema by specifying column types \r\n",
        "\r\n",
        "from azureml.data.dataset_factory import DataType\r\n",
        "\r\n",
        "titanic_ds = Dataset.Tabular.from_delimited_files(path=web_path, set_column_types={'Survived': DataType.to_bool()})\r\n",
        "\r\n",
        "# preview the first 3 rows of titanic_ds\r\n",
        "titanic_ds.take(3).to_pandas_dataframe()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "   PassengerId  Survived  Pclass  \\\n0            1     False       3   \n1            2      True       1   \n2            3      True       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500  None        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250  None        S  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>False</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>None</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>True</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>True</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>None</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1666026409328
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''import os\r\n",
        "# Create a folder for the script files\r\n",
        "script_folder = 'workshop_examples'\r\n",
        "os.makedirs(script_folder, exist_ok=True)\r\n",
        "\r\n",
        "print(script_folder)'''"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\r\n",
        "%%writefile $script_folder/train_titanic.py\r\n",
        "#access data in training script\r\n",
        "\r\n",
        "import argparse\r\n",
        "from azureml.core import Dataset, Run\r\n",
        "\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument(\"--input-data\", type=str)\r\n",
        "args = parser.parse_args()\r\n",
        "\r\n",
        "run = Run.get_context()\r\n",
        "ws = run.experiment.workspace\r\n",
        "\r\n",
        "# get the input dataset by ID\r\n",
        "dataset = Dataset.get_by_id(ws, id=args.input_data)\r\n",
        "\r\n",
        "# load the TabularDataset to pandas DataFrame\r\n",
        "df = dataset.to_pandas_dataframe()\r\n",
        "'''"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\r\n",
        "# define compute target, add the code\r\n",
        "# define environment as myenv., add the code\r\n",
        "from azureml.core import ScriptRunConfig\r\n",
        "\r\n",
        "src = ScriptRunConfig(source_directory=script_folder,\r\n",
        "                      script='train_titanic.py',\r\n",
        "                      # pass dataset as an input with friendly name 'titanic'\r\n",
        "                      arguments=['--input-data', titanic_ds.as_named_input('titanic')],# titanic_ds is given without registering as a dataset.\r\n",
        "                      compute_target=compute_target,\r\n",
        "                      environment=myenv)\r\n",
        "                             \r\n",
        "# Submit the run configuration for your training run\r\n",
        "run = experiment.submit(src)\r\n",
        "run.wait_for_completion(show_output=True)\r\n",
        "'''"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Tabular Dataset from Datastore (PARQUET files) and Register"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creates an unregistered TabularDataset from a DATASTORE  + PARQUET \r\n",
        "\r\n",
        "from azureml.core import Workspace, Datastore, Dataset\r\n",
        "\r\n",
        "datastore_name = ws.get_default_datastore()\r\n",
        "weather_ds = 'weather-data-florida'\r\n",
        "\r\n",
        "#load data to blob storage from local folder (weather-data) to be able to create a dataset\r\n",
        "datastore_name.upload('weather_data', weather_ds, overwrite=True, show_progress=True)\r\n",
        "\r\n",
        "datastore_path = [(datastore_name, weather_ds + '/*/*/data.parquet')]\r\n",
        "dataset        = Dataset.Tabular.from_parquet_files(path=datastore_path, partition_format = weather_ds + '/{partition_time:yyyy/MM}/data.parquet')\r\n",
        "# **** To read files in .csv or .tsv format, use  from_delimited_files() method. *****\r\n",
        "#dataset= Dataset.Tabular.from_delimited_files(path=datastore_path)\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Uploading an estimated of 12 files\nUploading weather_data/2019/01/.amlignore\nUploaded weather_data/2019/01/.amlignore, 1 files out of an estimated total of 12\nUploading weather_data/2019/01/.amlignore.amltmp\nUploaded weather_data/2019/01/.amlignore.amltmp, 2 files out of an estimated total of 12\nUploading weather_data/2019/01/data.parquet\nUploaded weather_data/2019/01/data.parquet, 3 files out of an estimated total of 12\nUploading weather_data/2019/02/data.parquet\nUploaded weather_data/2019/02/data.parquet, 4 files out of an estimated total of 12\nUploading weather_data/2019/03/data.parquet\nUploaded weather_data/2019/03/data.parquet, 5 files out of an estimated total of 12\nUploading weather_data/2019/04/data.parquet\nUploaded weather_data/2019/04/data.parquet, 6 files out of an estimated total of 12\nUploading weather_data/2019/05/data.parquet\nUploaded weather_data/2019/05/data.parquet, 7 files out of an estimated total of 12\nUploading weather_data/2019/06/data.parquet\nUploaded weather_data/2019/06/data.parquet, 8 files out of an estimated total of 12\nUploading weather_data/2019/07/data.parquet\nUploaded weather_data/2019/07/data.parquet, 9 files out of an estimated total of 12\nUploading weather_data/2019/08/data.parquet\nUploaded weather_data/2019/08/data.parquet, 10 files out of an estimated total of 12\nUploading weather_data/2019/09/data.parquet\nUploaded weather_data/2019/09/data.parquet, 11 files out of an estimated total of 12\nUploading weather_data/2019/10/data.parquet\nUploaded weather_data/2019/10/data.parquet, 12 files out of an estimated total of 12\nUploaded 12 files\n"
        }
      ],
      "execution_count": 39,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1666027959307
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.to_pandas_dataframe().head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 33,
          "data": {
            "text/plain": "     usaf   wban            datetime  latitude  longitude  elevation  \\\n0  720735  73805 2019-01-01 00:00:00    30.349    -85.788       21.0   \n1  720735  73805 2019-01-01 00:39:00    30.349    -85.788       21.0   \n2  720735  73805 2019-01-01 00:53:00    30.349    -85.788       21.0   \n3  720735  73805 2019-01-01 01:01:00    30.349    -85.788       21.0   \n4  720735  73805 2019-01-01 01:53:00    30.349    -85.788       21.0   \n\n   windAngle  windSpeed  temperature  seaLvlPressure  ... precipDepth  \\\n0      140.0        5.1         21.1             NaN  ...         NaN   \n1      150.0        5.7         21.1             NaN  ...         NaN   \n2      150.0        4.6         21.1          1019.5  ...         0.0   \n3      150.0        4.6         21.1             NaN  ...         NaN   \n4      140.0        4.1         21.1          1019.6  ...         0.0   \n\n   snowDepth                          stationName  countryOrRegion  \\\n0        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n1        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n2        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n3        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n4        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n\n            p_k  year day version __index_level_0__  partition_time  \n0  720735-73805  2019   1     1.0           2390756      2019-01-01  \n1  720735-73805  2019   1     1.0           2390757      2019-01-01  \n2  720735-73805  2019   1     1.0           2390758      2019-01-01  \n3  720735-73805  2019   1     1.0           2390759      2019-01-01  \n4  720735-73805  2019   1     1.0           2390760      2019-01-01  \n\n[5 rows x 24 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>usaf</th>\n      <th>wban</th>\n      <th>datetime</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>elevation</th>\n      <th>windAngle</th>\n      <th>windSpeed</th>\n      <th>temperature</th>\n      <th>seaLvlPressure</th>\n      <th>...</th>\n      <th>precipDepth</th>\n      <th>snowDepth</th>\n      <th>stationName</th>\n      <th>countryOrRegion</th>\n      <th>p_k</th>\n      <th>year</th>\n      <th>day</th>\n      <th>version</th>\n      <th>__index_level_0__</th>\n      <th>partition_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 00:00:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>140.0</td>\n      <td>5.1</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390756</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 00:39:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>5.7</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390757</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 00:53:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>4.6</td>\n      <td>21.1</td>\n      <td>1019.5</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390758</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 01:01:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>4.6</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390759</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 01:53:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>140.0</td>\n      <td>4.1</td>\n      <td>21.1</td>\n      <td>1019.6</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390760</td>\n      <td>2019-01-01</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 24 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1666026585701
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wrangle Data: Tabular Datasets\r\n",
        "* keep_columns()\r\n",
        "* drop_columns()\r\n",
        "* filter()\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\r\n",
        "tsd=dataset.filter(dataset['datetime']<datetime(2019,1,2))\r\n",
        "#tsd = tsd.time_after(datetime(2019, 1, 1)).time_before(datetime(2019, 1, 10))\r\n",
        "tsd.to_pandas_dataframe().head(5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Method filter: This is an experimental method, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 34,
          "data": {
            "text/plain": "     usaf   wban            datetime  latitude  longitude  elevation  \\\n0  720735  73805 2019-01-01 00:00:00    30.349    -85.788       21.0   \n1  720735  73805 2019-01-01 00:39:00    30.349    -85.788       21.0   \n2  720735  73805 2019-01-01 00:53:00    30.349    -85.788       21.0   \n3  720735  73805 2019-01-01 01:01:00    30.349    -85.788       21.0   \n4  720735  73805 2019-01-01 01:53:00    30.349    -85.788       21.0   \n\n   windAngle  windSpeed  temperature  seaLvlPressure  ... precipDepth  \\\n0      140.0        5.1         21.1             NaN  ...         NaN   \n1      150.0        5.7         21.1             NaN  ...         NaN   \n2      150.0        4.6         21.1          1019.5  ...         0.0   \n3      150.0        4.6         21.1             NaN  ...         NaN   \n4      140.0        4.1         21.1          1019.6  ...         0.0   \n\n   snowDepth                          stationName  countryOrRegion  \\\n0        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n1        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n2        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n3        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n4        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n\n            p_k  year day version __index_level_0__  partition_time  \n0  720735-73805  2019   1     1.0           2390756      2019-01-01  \n1  720735-73805  2019   1     1.0           2390757      2019-01-01  \n2  720735-73805  2019   1     1.0           2390758      2019-01-01  \n3  720735-73805  2019   1     1.0           2390759      2019-01-01  \n4  720735-73805  2019   1     1.0           2390760      2019-01-01  \n\n[5 rows x 24 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>usaf</th>\n      <th>wban</th>\n      <th>datetime</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>elevation</th>\n      <th>windAngle</th>\n      <th>windSpeed</th>\n      <th>temperature</th>\n      <th>seaLvlPressure</th>\n      <th>...</th>\n      <th>precipDepth</th>\n      <th>snowDepth</th>\n      <th>stationName</th>\n      <th>countryOrRegion</th>\n      <th>p_k</th>\n      <th>year</th>\n      <th>day</th>\n      <th>version</th>\n      <th>__index_level_0__</th>\n      <th>partition_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 00:00:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>140.0</td>\n      <td>5.1</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390756</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 00:39:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>5.7</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390757</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 00:53:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>4.6</td>\n      <td>21.1</td>\n      <td>1019.5</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390758</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 01:01:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>4.6</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390759</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 01:53:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>140.0</td>\n      <td>4.1</td>\n      <td>21.1</td>\n      <td>1019.6</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390760</td>\n      <td>2019-01-01</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 24 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1666027403761
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsd2 = tsd.keep_columns(columns=['snowDepth', 'datetime', 'partition_time'], validate=False)\r\n",
        "tsd2.to_pandas_dataframe().tail()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 35,
          "data": {
            "text/plain": "               datetime  snowDepth partition_time\n154 2019-01-01 20:00:00        NaN     2019-01-01\n155 2019-01-01 21:00:00        NaN     2019-01-01\n156 2019-01-01 21:00:00        NaN     2019-01-01\n157 2019-01-01 22:00:00        NaN     2019-01-01\n158 2019-01-01 23:00:00        NaN     2019-01-01",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>datetime</th>\n      <th>snowDepth</th>\n      <th>partition_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>154</th>\n      <td>2019-01-01 20:00:00</td>\n      <td>NaN</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>155</th>\n      <td>2019-01-01 21:00:00</td>\n      <td>NaN</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>156</th>\n      <td>2019-01-01 21:00:00</td>\n      <td>NaN</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>157</th>\n      <td>2019-01-01 22:00:00</td>\n      <td>NaN</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>158</th>\n      <td>2019-01-01 23:00:00</td>\n      <td>NaN</td>\n      <td>2019-01-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1666027409500
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsd2 = tsd.drop_columns(columns=['wban', 'snowDepth'])\r\n",
        "tsd2.take(5).to_pandas_dataframe()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 36,
          "data": {
            "text/plain": "     usaf            datetime  latitude  longitude  elevation  windAngle  \\\n0  720735 2019-01-01 00:00:00    30.349    -85.788       21.0      140.0   \n1  720735 2019-01-01 00:39:00    30.349    -85.788       21.0      150.0   \n2  720735 2019-01-01 00:53:00    30.349    -85.788       21.0      150.0   \n3  720735 2019-01-01 01:01:00    30.349    -85.788       21.0      150.0   \n4  720735 2019-01-01 01:53:00    30.349    -85.788       21.0      140.0   \n\n   windSpeed  temperature  seaLvlPressure cloudCoverage  ... precipTime  \\\n0        5.1         21.1             NaN          None  ...        NaN   \n1        5.7         21.1             NaN          None  ...        NaN   \n2        4.6         21.1          1019.5          None  ...        1.0   \n3        4.6         21.1             NaN          None  ...        NaN   \n4        4.1         21.1          1019.6          None  ...        1.0   \n\n  precipDepth                          stationName  countryOrRegion  \\\n0         NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n1         NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n2         0.0  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n3         NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n4         0.0  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n\n            p_k  year day  version  __index_level_0__  partition_time  \n0  720735-73805  2019   1      1.0            2390756      2019-01-01  \n1  720735-73805  2019   1      1.0            2390757      2019-01-01  \n2  720735-73805  2019   1      1.0            2390758      2019-01-01  \n3  720735-73805  2019   1      1.0            2390759      2019-01-01  \n4  720735-73805  2019   1      1.0            2390760      2019-01-01  \n\n[5 rows x 22 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>usaf</th>\n      <th>datetime</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>elevation</th>\n      <th>windAngle</th>\n      <th>windSpeed</th>\n      <th>temperature</th>\n      <th>seaLvlPressure</th>\n      <th>cloudCoverage</th>\n      <th>...</th>\n      <th>precipTime</th>\n      <th>precipDepth</th>\n      <th>stationName</th>\n      <th>countryOrRegion</th>\n      <th>p_k</th>\n      <th>year</th>\n      <th>day</th>\n      <th>version</th>\n      <th>__index_level_0__</th>\n      <th>partition_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>720735</td>\n      <td>2019-01-01 00:00:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>140.0</td>\n      <td>5.1</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390756</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>720735</td>\n      <td>2019-01-01 00:39:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>5.7</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390757</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>720735</td>\n      <td>2019-01-01 00:53:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>4.6</td>\n      <td>21.1</td>\n      <td>1019.5</td>\n      <td>None</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390758</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>720735</td>\n      <td>2019-01-01 01:01:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>4.6</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390759</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>720735</td>\n      <td>2019-01-01 01:53:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>140.0</td>\n      <td>4.1</td>\n      <td>21.1</td>\n      <td>1019.6</td>\n      <td>None</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390760</td>\n      <td>2019-01-01</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1666027415380
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Register Tabular Dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can register a new dataset under the same name by creating a new version. A dataset version is a way to bookmark the state of your data so that you can apply a specific version of the dataset for experimentation or future reproduction."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign \"datetime\" column as timestamp and \"partition_time\" from folder path as partition_timestamp \r\n",
        "# for Tabular Dataset to activate Time Series related APIs. The column to be assigned should be a Date type.\r\n",
        "#dataset = dataset.with_timestamp_columns(timestamp='datetime', partition_timestamp='partition_time')\r\n",
        "\r\n",
        "# register dataset to Workspace\r\n",
        "registered_ds = dataset.register(workspace=ws, \r\n",
        "                            name=weather_ds, \r\n",
        "                            create_new_version=True, \r\n",
        "                            description='Data for Tabular Dataset- time-series.', \r\n",
        "                            tags={ 'type': 'TabularDataset' })"
      ],
      "outputs": [],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1666029532021
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "registered_ds.to_pandas_dataframe().head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 45,
          "data": {
            "text/plain": "     usaf   wban            datetime  latitude  longitude  elevation  \\\n0  720735  73805 2019-01-01 00:00:00    30.349    -85.788       21.0   \n1  720735  73805 2019-01-01 00:39:00    30.349    -85.788       21.0   \n2  720735  73805 2019-01-01 00:53:00    30.349    -85.788       21.0   \n3  720735  73805 2019-01-01 01:01:00    30.349    -85.788       21.0   \n4  720735  73805 2019-01-01 01:53:00    30.349    -85.788       21.0   \n\n   windAngle  windSpeed  temperature  seaLvlPressure  ... precipDepth  \\\n0      140.0        5.1         21.1             NaN  ...         NaN   \n1      150.0        5.7         21.1             NaN  ...         NaN   \n2      150.0        4.6         21.1          1019.5  ...         0.0   \n3      150.0        4.6         21.1             NaN  ...         NaN   \n4      140.0        4.1         21.1          1019.6  ...         0.0   \n\n   snowDepth                          stationName  countryOrRegion  \\\n0        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n1        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n2        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n3        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n4        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n\n            p_k  year day version __index_level_0__  partition_time  \n0  720735-73805  2019   1     1.0           2390756      2019-01-01  \n1  720735-73805  2019   1     1.0           2390757      2019-01-01  \n2  720735-73805  2019   1     1.0           2390758      2019-01-01  \n3  720735-73805  2019   1     1.0           2390759      2019-01-01  \n4  720735-73805  2019   1     1.0           2390760      2019-01-01  \n\n[5 rows x 24 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>usaf</th>\n      <th>wban</th>\n      <th>datetime</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>elevation</th>\n      <th>windAngle</th>\n      <th>windSpeed</th>\n      <th>temperature</th>\n      <th>seaLvlPressure</th>\n      <th>...</th>\n      <th>precipDepth</th>\n      <th>snowDepth</th>\n      <th>stationName</th>\n      <th>countryOrRegion</th>\n      <th>p_k</th>\n      <th>year</th>\n      <th>day</th>\n      <th>version</th>\n      <th>__index_level_0__</th>\n      <th>partition_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 00:00:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>140.0</td>\n      <td>5.1</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390756</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 00:39:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>5.7</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390757</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 00:53:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>4.6</td>\n      <td>21.1</td>\n      <td>1019.5</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390758</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 01:01:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>4.6</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390759</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 01:53:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>140.0</td>\n      <td>4.1</td>\n      <td>21.1</td>\n      <td>1019.6</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390760</td>\n      <td>2019-01-01</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 24 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 45,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1666029545530
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reload the Dataset from Workspace"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get dataset by dataset name\r\n",
        "tsd = Dataset.get_by_name(ws, name=weather_ds)\r\n",
        "#get_by_id() \r\n",
        "\r\n",
        "tsd.to_pandas_dataframe().head(5)\r\n",
        "#returns the latest version of the dataset"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 46,
          "data": {
            "text/plain": "     usaf   wban            datetime  latitude  longitude  elevation  \\\n0  720735  73805 2019-01-01 00:00:00    30.349    -85.788       21.0   \n1  720735  73805 2019-01-01 00:39:00    30.349    -85.788       21.0   \n2  720735  73805 2019-01-01 00:53:00    30.349    -85.788       21.0   \n3  720735  73805 2019-01-01 01:01:00    30.349    -85.788       21.0   \n4  720735  73805 2019-01-01 01:53:00    30.349    -85.788       21.0   \n\n   windAngle  windSpeed  temperature  seaLvlPressure  ... precipDepth  \\\n0      140.0        5.1         21.1             NaN  ...         NaN   \n1      150.0        5.7         21.1             NaN  ...         NaN   \n2      150.0        4.6         21.1          1019.5  ...         0.0   \n3      150.0        4.6         21.1             NaN  ...         NaN   \n4      140.0        4.1         21.1          1019.6  ...         0.0   \n\n   snowDepth                          stationName  countryOrRegion  \\\n0        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n1        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n2        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n3        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n4        NaN  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n\n            p_k  year day version __index_level_0__  partition_time  \n0  720735-73805  2019   1     1.0           2390756      2019-01-01  \n1  720735-73805  2019   1     1.0           2390757      2019-01-01  \n2  720735-73805  2019   1     1.0           2390758      2019-01-01  \n3  720735-73805  2019   1     1.0           2390759      2019-01-01  \n4  720735-73805  2019   1     1.0           2390760      2019-01-01  \n\n[5 rows x 24 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>usaf</th>\n      <th>wban</th>\n      <th>datetime</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>elevation</th>\n      <th>windAngle</th>\n      <th>windSpeed</th>\n      <th>temperature</th>\n      <th>seaLvlPressure</th>\n      <th>...</th>\n      <th>precipDepth</th>\n      <th>snowDepth</th>\n      <th>stationName</th>\n      <th>countryOrRegion</th>\n      <th>p_k</th>\n      <th>year</th>\n      <th>day</th>\n      <th>version</th>\n      <th>__index_level_0__</th>\n      <th>partition_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 00:00:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>140.0</td>\n      <td>5.1</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390756</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 00:39:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>5.7</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390757</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 00:53:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>4.6</td>\n      <td>21.1</td>\n      <td>1019.5</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390758</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 01:01:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>4.6</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390759</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 01:53:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>140.0</td>\n      <td>4.1</td>\n      <td>21.1</td>\n      <td>1019.6</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390760</td>\n      <td>2019-01-01</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 24 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 46,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1666029725675
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a dataset from pandas dataframe"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Datastore, Dataset\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "pandas_df = pd.read_csv('https://ml.azure.com/fileexplorerAzNB?wsid=/subscriptions/fe38c376-b42a-4741-9e7c-f5d7c31e5873/resourcegroups/ProdRG/providers/Microsoft.MachineLearningServices/workspaces/Prod&tid=16b3c013-d300-468d-ac64-7eda0820b6d3&activeFilePath=Users/yelizkilinc/automl-workshop/Data%20Basics/workshop_examples/Data/inference_data')\r\n",
        "\r\n",
        "datastore = Datastore.get(ws, 'workspaceblobstore')\r\n",
        "dataset = Dataset.Tabular.register_pandas_dataframe(pandas_df, datastore, \"dataset_from_pandas_df\", show_progress=True)\r\n",
        "dataset.take(3).to_pandas_dataframe()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Validating arguments.\nArguments validated.\nSuccessfully obtained datastore reference and path.\nUploading file to managed-dataset/df416d89-5001-493c-9301-86bbb57c81a7/\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: '<!doctype html><html lang=\"en\" class=\"notranslate\" translate=\"no\"><head><meta charset=\"utf-8\"/><meta name=\"Description\" content=\"Azure Machine Learning Studio is a GUI-based integrated development environment for constructing and operationalizing Machine Learning workflow on Azure.\"/><link rel=\"shortcut icon\" href=\"/favicon.ico\"/><title>Microsoft Azure Machine Learning Studio</title><script>window.App_Version=\"WorkspacePortal-2022-Oct-10-2022-10-13.1\"' -> '<!doctype html><html lang=\"en\" class=\"notranslate\" translate=\"no\"><head><meta charset=\"utf-8\"/><meta name=\"Description\" content=\"Azure Machine Learning Studio is a GUI-based integrated development environment for constructing and operationalizing Machine Learning workflow on Azure_\"/><link rel=\"shortcut icon\" href=\"/favicon_ico\"/><title>Microsoft Azure Machine Learning Studio</title><script>window_App_Version=\"WorkspacePortal-2022-Oct-10-2022-10-13_1\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'window.App_Region=\"uksouth\"' -> 'window_App_Region=\"uksouth\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'window.Cloud_Api_Host=\"uksouth.api.azureml.ms\"' -> 'window_Cloud_Api_Host=\"uksouth_api_azureml_ms\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'window.App_InstrumentationKey=\"##APPINSIGHTS_INSTRUMENTATIONKEY##\"</script><script>try{const e=performance.now()' -> 'window_App_InstrumentationKey=\"##APPINSIGHTS_INSTRUMENTATIONKEY##\"</script><script>try{const e=performance_now()'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 't=indexedDB.open(\"keyval-store\");t.onupgradeneeded=()=>t.result.createObjectStore(storeName)' -> 't=indexedDB_open(\"keyval-store\");t_onupgradeneeded=()=>t_result_createObjectStore(storeName)'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 't.onsuccess=t=>{t.target.result.transaction(\"keyval\"' -> 't_onsuccess=t=>{t_target_result_transaction(\"keyval\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'readonly).objectStore(\"keyval\").get(\"REACT_QUERY_OFFLINE_CACHE\").onsuccess=t=>{try{window.InitialReactQueryCacheData=JSON.parse(t.target.result||\"{}\")' -> 'readonly)_objectStore(\"keyval\")_get(\"REACT_QUERY_OFFLINE_CACHE\")_onsuccess=t=>{try{window_InitialReactQueryCacheData=JSON_parse(t_target_result||\"{}\")'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'console.log(`React query cache restored in ${performance.now()-e} ms`)}catch{}}}}catch{}</script><link href=\"/static/css/styles.a63385fc.chunk.css\" rel=\"stylesheet\"></head><body id=\"mlworkspace\"><div id=\"root\"></div><script>!function(e){function a(a){for(var b' -> 'console_log(`React query cache restored in ${performance_now()-e} ms`)}catch{}}}}catch{}</script><link href=\"/static/css/styles_a63385fc_chunk_css\" rel=\"stylesheet\"></head><body id=\"mlworkspace\"><div id=\"root\"></div><script>!function(e){function a(a){for(var b'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'u=[];p<o.length;p++)r=o[p]' -> 'u=[];p<o_length;p++)r=o[p]'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'Object.prototype.hasOwnProperty.call(f' -> 'Object_prototype_hasOwnProperty_call(f'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'r)&&f[r]&&u.push(f[r][0])' -> 'r)&&f[r]&&u_push(f[r][0])'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'f[r]=0;for(b in n)Object.prototype.hasOwnProperty.call(n' -> 'f[r]=0;for(b in n)Object_prototype_hasOwnProperty_call(n'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 't.push.apply(t' -> 't_push_apply(t'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'i);u.length;)u.shift()();return d.push.apply(d' -> 'i);u_length;)u_shift()();return d_push_apply(d'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'a=0;a<d.length;a++){for(var c=d[a]' -> 'a=0;a<d_length;a++){for(var c=d[a]'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'n=1;n<c.length;n++){var s=c[n];0!==f[s]&&(b=!1)}b&&(d.splice(a--' -> 'n=1;n<c_length;n++){var s=c[n];0!==f[s]&&(b=!1)}b&&(d_splice(a--'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'e=o(o.s=c[0]))}return 0===d.length&&(t.forEach((function(e){if(void 0===f[e]){f[e]=null;var a=document.createElement(\"link\");o.nc&&a.setAttribute(\"nonce\"' -> 'e=o(o_s=c[0]))}return 0===d_length&&(t_forEach((function(e){if(void 0===f[e]){f[e]=null;var a=document_createElement(\"link\");o_nc&&a_setAttribute(\"nonce\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'o.nc)' -> 'o_nc)'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'a.rel=\"prefetch\"' -> 'a_rel=\"prefetch\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'a.as=\"script\"' -> 'a_as=\"script\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'a.href=r(e)' -> 'a_href=r(e)'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'document.head.appendChild(a)}}))' -> 'document_head_appendChild(a)}}))'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 't.length=0)' -> 't_length=0)'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 't=[];function r(e){return o.p+\"static/js/\"+({2:\"monacoeditor\"' -> 't=[];function r(e){return o_p+\"static/js/\"+({2:\"monacoeditor\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: '472:\"visualInterface-home-data-provider\"}[e]||e)+\".\"+{2:\"688f9992\"' -> '472:\"visualInterface-home-data-provider\"}[e]||e)+\"_\"+{2:\"688f9992\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: '2102:\"cbb606e3\"}[e]+\".chunk.js\"}function o(a){if(b[a])return b[a].exports;var c=b[a]={i:a' -> '2102:\"cbb606e3\"}[e]+\"_chunk_js\"}function o(a){if(b[a])return b[a]_exports;var c=b[a]={i:a'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'exports:{}};return e[a].call(c.exports' -> 'exports:{}};return e[a]_call(c_exports'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'c.exports' -> 'c_exports'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'c.l=!0' -> 'c_l=!0'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'c.exports}o.e=function(e){var a=[]' -> 'c_exports}o_e=function(e){var a=[]'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'c=f[e];if(0!==c)if(c)a.push(c[2]);else{var b=new Promise((function(a' -> 'c=f[e];if(0!==c)if(c)a_push(c[2]);else{var b=new Promise((function(a'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'b]}));a.push(c[2]=b);var d=new Error' -> 'b]}));a_push(c[2]=b);var d=new Error'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'n=document.createElement(\"script\")' -> 'n=document_createElement(\"script\")'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 's=\"&retry-attempt=\"+(3-b+1);n.charset=\"utf-8\"' -> 's=\"&retry-attempt=\"+(3-b+1);n_charset=\"utf-8\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'n.timeout=120' -> 'n_timeout=120'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'o.nc&&n.setAttribute(\"nonce\"' -> 'o_nc&&n_setAttribute(\"nonce\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'o.nc).1' -> 'o_nc)_1'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'n.src=c' -> 'n_src=c'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 't=function(c){n.onerror=n.onload=null' -> 't=function(c){n_onerror=n_onload=null'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'clearTimeout(i);var t=f[e];if(0!==t)if(t)if(0===b){var o=c&&(\"load\"===c.type?\"missing\":c.type)' -> 'clearTimeout(i);var t=f[e];if(0!==t)if(t)if(0===b){var o=c&&(\"load\"===c_type?\"missing\":c_type)'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'l=c&&c.target&&c.target.src;d.message=\"Loading chunk \"+e+\" failed after 3 retries.\\n(\"+o+\": \"+l+\")\"' -> 'l=c&&c_target&&c_target_src;d_message=\"Loading chunk \"+e+\" failed after 3 retries_\\n(\"+o+\": \"+l+\")\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'd.name=\"ChunkLoadError\"' -> 'd_name=\"ChunkLoadError\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'd.type=o' -> 'd_type=o'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'd.request=l' -> 'd_request=l'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'window.chunkLoadFailed=!0;const a=new URL(window.location.href)' -> 'window_chunkLoadFailed=!0;const a=new URL(window_location_href)'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'b=a.searchParams.get(\"reloadCount\")' -> 'b=a_searchParams_get(\"reloadCount\")'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'r=b?parseInt(b):0;r<3&&(a.searchParams.set(\"reloadCount\"' -> 'r=b?parseInt(b):0;r<3&&(a_searchParams_set(\"reloadCount\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'window.location=a)' -> 'window_location=a)'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'b-1);document.head.appendChild(u)}else f[e]=void 0};var i=setTimeout((function(){t({type:\"timeout\"' -> 'b-1);document_head_appendChild(u)}else f[e]=void 0};var i=setTimeout((function(){t({type:\"timeout\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: '12e4);return n.onerror=n.onload=t' -> '12e4);return n_onerror=n_onload=t'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: '3);document.head.appendChild(t)}var n={446:[0' -> '3);document_head_appendChild(t)}var n={446:[0'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: '449]}[e];return n&&n.forEach((function(e){if(void 0===f[e]){f[e]=null;var a=document.createElement(\"link\");a.charset=\"utf-8\"' -> '449]}[e];return n&&n_forEach((function(e){if(void 0===f[e]){f[e]=null;var a=document_createElement(\"link\");a_charset=\"utf-8\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'o.nc&&a.setAttribute(\"nonce\"' -> 'o_nc&&a_setAttribute(\"nonce\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'o.nc).2' -> 'o_nc)_2'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'a.rel=\"preload\"' -> 'a_rel=\"preload\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'a.as=\"script\".1' -> 'a_as=\"script\"_1'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'a.href=r(e).1' -> 'a_href=r(e)_1'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'document.head.appendChild(a)}})).1' -> 'document_head_appendChild(a)}}))_1'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'Promise.all(a)}' -> 'Promise_all(a)}'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'o.m=e' -> 'o_m=e'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'o.c=b' -> 'o_c=b'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'o.d=function(e' -> 'o_d=function(e'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'c){o.o(e' -> 'c){o_o(e'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'a)||Object.defineProperty(e' -> 'a)||Object_defineProperty(e'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'a.1' -> 'a_1'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'o.r=function(e){\"undefined\"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e' -> 'o_r=function(e){\"undefined\"!=typeof Symbol&&Symbol_toStringTag&&Object_defineProperty(e'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'Symbol.toStringTag' -> 'Symbol_toStringTag'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'Object.defineProperty(e' -> 'Object_defineProperty(e'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'o.t=function(e' -> 'o_t=function(e'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: '8&a)return e;if(4&a&&\"object\"==typeof e&&e&&e.__esModule)return e;var c=Object.create(null);if(o.r(c)' -> '8&a)return e;if(4&a&&\"object\"==typeof e&&e&&e___esModule)return e;var c=Object_create(null);if(o_r(c)'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'Object.defineProperty(c' -> 'Object_defineProperty(c'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: '{enumerable:!0.1' -> '{enumerable:!0_1'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: '2&a&&\"string\"!=typeof e)for(var b in e)o.d(c' -> '2&a&&\"string\"!=typeof e)for(var b in e)o_d(c'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'function(a){return e[a]}.bind(null' -> 'function(a){return e[a]}_bind(null'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'o.n=function(e){var a=e&&e.__esModule?function(){return e.default}:function(){return e};return o.d(a' -> 'o_n=function(e){var a=e&&e___esModule?function(){return e_default}:function(){return e};return o_d(a'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'a.2' -> 'a_2'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'o.o=function(e' -> 'o_o=function(e'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'a){return Object.prototype.hasOwnProperty.call(e' -> 'a){return Object_prototype_hasOwnProperty_call(e'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'o.p=\"/\"' -> 'o_p=\"/\"'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'o.oe=function(e){throw console.error(e)' -> 'o_oe=function(e){throw console_error(e)'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 's=n.push.bind(n);n.push=a' -> 's=n_push_bind(n);n_push=a'\nColumn header contains '.' This period will be translated to '_' as we write the data out to parquet files: 'n=n.slice();for(var i=0;i<n.length;i++)a(n[i]);var l=s;c()}([])</script><script src=\"/static/js/styles.165f4c27.chunk.js\"></script><script src=\"/static/js/fluentui.1cdbe1d8.chunk.js\"></script><script src=\"/static/js/react.589c0d93.chunk.js\"></script><script src=\"/static/js/index.2af38d3b.chunk.js\"></script></body></html>' -> 'n=n_slice();for(var i=0;i<n_length;i++)a(n[i]);var l=s;c()}([])</script><script src=\"/static/js/styles_165f4c27_chunk_js\"></script><script src=\"/static/js/fluentui_1cdbe1d8_chunk_js\"></script><script src=\"/static/js/react_589c0d93_chunk_js\"></script><script src=\"/static/js/index_2af38d3b_chunk_js\"></script></body></html>'\nSuccessfully uploaded file to datastore.\nCreating and registering a new dataset.\n"
        },
        {
          "output_type": "error",
          "ename": "DatasetValidationError",
          "evalue": "DatasetValidationError:\n\tMessage: Failed to validate the data.\nThe Dataflow produced no records.| session_id=995f453d-1638-4dc7-9737-8fa9d8296a51\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Failed to validate the data.\\nThe Dataflow produced no records.| session_id=995f453d-1638-4dc7-9737-8fa9d8296a51\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDataflowValidationError\u001b[0m                   Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/data/dataset_error_handling.py:65\u001b[0m, in \u001b[0;36m_validate_has_data\u001b[0;34m(dataflow, error_message)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mdataflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify_has_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (dataprep()\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mdataflow\u001b[38;5;241m.\u001b[39mDataflowValidationError,\n\u001b[1;32m     67\u001b[0m         dataprep()\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39merrorhandlers\u001b[38;5;241m.\u001b[39mExecutionError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/dataprep/api/_loggerfactory.py:219\u001b[0m, in \u001b[0;36mtrack.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/dataprep/api/dataflow.py:845\u001b[0m, in \u001b[0;36mDataflow.verify_has_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39m_to_pyrecords()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DataflowValidationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Dataflow produced no records.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mDataflowValidationError\u001b[0m: \nError Code: Empty\nError Message: The Dataflow produced no records.| session_id=995f453d-1638-4dc7-9737-8fa9d8296a51",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDatasetValidationError\u001b[0m                    Traceback (most recent call last)",
            "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m pandas_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://ml.azure.com/fileexplorerAzNB?wsid=/subscriptions/fe38c376-b42a-4741-9e7c-f5d7c31e5873/resourcegroups/ProdRG/providers/Microsoft.MachineLearningServices/workspaces/Prod&tid=16b3c013-d300-468d-ac64-7eda0820b6d3&activeFilePath=Users/yelizkilinc/automl-workshop/Data\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m20Basics/workshop_examples/Data/inference_data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m datastore \u001b[38;5;241m=\u001b[39m Datastore\u001b[38;5;241m.\u001b[39mget(ws, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkspaceblobstore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTabular\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_pandas_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpandas_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatastore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_from_pandas_df\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto_pandas_dataframe()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/data/_loggerfactory.py:132\u001b[0m, in \u001b[0;36mtrack.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _LoggerFactory\u001b[38;5;241m.\u001b[39mtrack_activity(logger, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions) \u001b[38;5;28;01mas\u001b[39;00m al:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(al, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity_info\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_code\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/data/dataset_factory.py:660\u001b[0m, in \u001b[0;36mTabularDatasetFactory.register_pandas_dataframe\u001b[0;34m(dataframe, target, name, description, tags, show_progress)\u001b[0m\n\u001b[1;32m    658\u001b[0m console(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating and registering a new dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    659\u001b[0m datapath \u001b[38;5;241m=\u001b[39m DataPath(datastore, relative_path_with_guid)\n\u001b[0;32m--> 660\u001b[0m saved_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTabularDatasetFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_parquet_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatapath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    661\u001b[0m registered_dataset \u001b[38;5;241m=\u001b[39m saved_dataset\u001b[38;5;241m.\u001b[39mregister(datastore\u001b[38;5;241m.\u001b[39mworkspace, name,\n\u001b[1;32m    662\u001b[0m                                             description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[1;32m    663\u001b[0m                                             tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    664\u001b[0m                                             create_new_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    665\u001b[0m console(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully created and registered a new dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/data/_loggerfactory.py:132\u001b[0m, in \u001b[0;36mtrack.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _LoggerFactory\u001b[38;5;241m.\u001b[39mtrack_activity(logger, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions) \u001b[38;5;28;01mas\u001b[39;00m al:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(al, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity_info\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_code\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/data/dataset_factory.py:142\u001b[0m, in \u001b[0;36mTabularDatasetFactory.from_parquet_files\u001b[0;34m(path, validate, include_path, set_column_types, partition_format)\u001b[0m\n\u001b[1;32m    140\u001b[0m path \u001b[38;5;241m=\u001b[39m _validate_and_normalize_path(path)\n\u001b[1;32m    141\u001b[0m _trace_dataset_creation(path)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTabularDatasetFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_parquet_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43minclude_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mset_column_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mpartition_format\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/data/dataset_factory.py:156\u001b[0m, in \u001b[0;36mTabularDatasetFactory._from_parquet_files\u001b[0;34m(path, validate, include_path, set_column_types, partition_format)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazureml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TabularDataset\n\u001b[1;32m    153\u001b[0m dataflow \u001b[38;5;241m=\u001b[39m dataprep()\u001b[38;5;241m.\u001b[39mread_parquet_file(path,\n\u001b[1;32m    154\u001b[0m                                         include_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m                                         verify_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 156\u001b[0m dataflow \u001b[38;5;241m=\u001b[39m \u001b[43m_transform_and_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_is_inference_required\u001b[49m\u001b[43m(\u001b[49m\u001b[43mset_column_types\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m dataflow \u001b[38;5;241m=\u001b[39m _set_column_types(dataflow, set_column_types)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TabularDataset\u001b[38;5;241m.\u001b[39m_create(dataflow)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/data/dataset_factory.py:1172\u001b[0m, in \u001b[0;36m_transform_and_validate\u001b[0;34m(dataflow, partition_format, include_path, validate, infer_column_types)\u001b[0m\n\u001b[1;32m   1170\u001b[0m     dataflow \u001b[38;5;241m=\u001b[39m dataflow\u001b[38;5;241m.\u001b[39mdrop_columns(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPath\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate:\n\u001b[0;32m-> 1172\u001b[0m     \u001b[43m_validate_has_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFailed to validate the data.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m infer_column_types:\n\u001b[1;32m   1174\u001b[0m     _validate_has_data(dataflow, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to infer column type.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1175\u001b[0m                                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mif data is inaccessible, please set infer_column_types to False.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/data/dataset_error_handling.py:68\u001b[0m, in \u001b[0;36m_validate_has_data\u001b[0;34m(dataflow, error_message)\u001b[0m\n\u001b[1;32m     65\u001b[0m     dataflow\u001b[38;5;241m.\u001b[39mverify_has_data()\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (dataprep()\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mdataflow\u001b[38;5;241m.\u001b[39mDataflowValidationError,\n\u001b[1;32m     67\u001b[0m         dataprep()\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39merrorhandlers\u001b[38;5;241m.\u001b[39mExecutionError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetValidationError(error_message \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m e\u001b[38;5;241m.\u001b[39mcompliant_message, exception\u001b[38;5;241m=\u001b[39me)\n",
            "\u001b[0;31mDatasetValidationError\u001b[0m: DatasetValidationError:\n\tMessage: Failed to validate the data.\nThe Dataflow produced no records.| session_id=995f453d-1638-4dc7-9737-8fa9d8296a51\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Failed to validate the data.\\nThe Dataflow produced no records.| session_id=995f453d-1638-4dc7-9737-8fa9d8296a51\"\n    }\n}"
          ]
        }
      ],
      "execution_count": 53,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1666030199235
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Method filter: This is an experimental method, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "                       usaf   wban            datetime  latitude  longitude  \\\n2019-01-01 00:00:00  720735  73805 2019-01-01 00:00:00    30.349    -85.788   \n2019-01-01 00:39:00  720735  73805 2019-01-01 00:39:00    30.349    -85.788   \n2019-01-01 00:53:00  720735  73805 2019-01-01 00:53:00    30.349    -85.788   \n2019-01-01 01:01:00  720735  73805 2019-01-01 01:01:00    30.349    -85.788   \n2019-01-01 01:53:00  720735  73805 2019-01-01 01:53:00    30.349    -85.788   \n\n                     elevation  windAngle  windSpeed  temperature  \\\n2019-01-01 00:00:00       21.0      140.0        5.1         21.1   \n2019-01-01 00:39:00       21.0      150.0        5.7         21.1   \n2019-01-01 00:53:00       21.0      150.0        4.6         21.1   \n2019-01-01 01:01:00       21.0      150.0        4.6         21.1   \n2019-01-01 01:53:00       21.0      140.0        4.1         21.1   \n\n                     seaLvlPressure  ... precipDepth  snowDepth  \\\n2019-01-01 00:00:00             NaN  ...         NaN        NaN   \n2019-01-01 00:39:00             NaN  ...         NaN        NaN   \n2019-01-01 00:53:00          1019.5  ...         0.0        NaN   \n2019-01-01 01:01:00             NaN  ...         NaN        NaN   \n2019-01-01 01:53:00          1019.6  ...         0.0        NaN   \n\n                                             stationName  countryOrRegion  \\\n2019-01-01 00:00:00  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n2019-01-01 00:39:00  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n2019-01-01 00:53:00  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n2019-01-01 01:01:00  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n2019-01-01 01:53:00  NORTHWEST FLORIDA BEACHES INTL ARPT               US   \n\n                              p_k  year day version __index_level_0__  \\\n2019-01-01 00:00:00  720735-73805  2019   1     1.0           2390756   \n2019-01-01 00:39:00  720735-73805  2019   1     1.0           2390757   \n2019-01-01 00:53:00  720735-73805  2019   1     1.0           2390758   \n2019-01-01 01:01:00  720735-73805  2019   1     1.0           2390759   \n2019-01-01 01:53:00  720735-73805  2019   1     1.0           2390760   \n\n                     partition_time  \n2019-01-01 00:00:00      2019-01-01  \n2019-01-01 00:39:00      2019-01-01  \n2019-01-01 00:53:00      2019-01-01  \n2019-01-01 01:01:00      2019-01-01  \n2019-01-01 01:53:00      2019-01-01  \n\n[5 rows x 24 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>usaf</th>\n      <th>wban</th>\n      <th>datetime</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>elevation</th>\n      <th>windAngle</th>\n      <th>windSpeed</th>\n      <th>temperature</th>\n      <th>seaLvlPressure</th>\n      <th>...</th>\n      <th>precipDepth</th>\n      <th>snowDepth</th>\n      <th>stationName</th>\n      <th>countryOrRegion</th>\n      <th>p_k</th>\n      <th>year</th>\n      <th>day</th>\n      <th>version</th>\n      <th>__index_level_0__</th>\n      <th>partition_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-01-01 00:00:00</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 00:00:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>140.0</td>\n      <td>5.1</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390756</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>2019-01-01 00:39:00</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 00:39:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>5.7</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390757</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>2019-01-01 00:53:00</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 00:53:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>4.6</td>\n      <td>21.1</td>\n      <td>1019.5</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390758</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>2019-01-01 01:01:00</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 01:01:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>150.0</td>\n      <td>4.6</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390759</td>\n      <td>2019-01-01</td>\n    </tr>\n    <tr>\n      <th>2019-01-01 01:53:00</th>\n      <td>720735</td>\n      <td>73805</td>\n      <td>2019-01-01 01:53:00</td>\n      <td>30.349</td>\n      <td>-85.788</td>\n      <td>21.0</td>\n      <td>140.0</td>\n      <td>4.1</td>\n      <td>21.1</td>\n      <td>1019.6</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NORTHWEST FLORIDA BEACHES INTL ARPT</td>\n      <td>US</td>\n      <td>720735-73805</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2390760</td>\n      <td>2019-01-01</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 24 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665837440766
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create File Dataset **"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use from_files() method to load files in any format and create un registered filedataset."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Represents a collection of file references in datastores or public URLs to use in Azure Machine Learning.\r\n",
        "\r\n",
        "from azureml.core.dataset import Dataset\r\n",
        "\r\n",
        "web_paths = [\r\n",
        "            'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\r\n",
        "            'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\r\n",
        "            'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\r\n",
        "            'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\r\n",
        "            ]\r\n",
        "\r\n",
        "mnist_ds = Dataset.File.from_files(path = web_paths)\r\n",
        "mnist_ds.to_path() # list the files"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": "['/http%3A/%2Fyann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n '/http%3A/%2Fyann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n '/http%3A/%2Fyann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n '/http%3A/%2Fyann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665839968754
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "mnist_file_dataset = mnist_ds.register(workspace=ws,\r\n",
        "                                        name='mnist_opendataset',\r\n",
        "                                        description='mnist training and test dataset',\r\n",
        "                                        create_new_version=True)"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665840675708
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create dataset from datastore"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "{\n  \"source\": [\n    \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n    \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n    \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n    \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\"\n  ],\n  \"definition\": [\n    \"GetFiles\"\n  ]\n}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665838391946
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Access registered filedataset "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ??? Create adls gen2 storage ??\r\n",
        "'''# Create Azure Data Lake Storage Gen2 datastore without credentials.\r\n",
        "adls2_dstore = Datastore.register_azure_data_lake_gen2(workspace=ws, \r\n",
        "                                                       datastore_name='credentialless_adls2', \r\n",
        "                                                       filesystem='tabular', \r\n",
        "                                                       account_name='myadls2')'''"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve an existing datastore in the workspace by name \r\n",
        "dstore = Datastore.get(workspace, datastore_name) \r\n",
        "print(dstore)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}