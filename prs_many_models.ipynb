{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Licensed under the MIT License."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-partition-per-column.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Azure Machine Learning Pipelines and Parallel Run Step (PRS) for Many Model Training and Batch Inference using tabular input partitioned by column value\r\n",
        "\r\n",
        "\r\n",
        "This example will create a partitioned tabular dataset by splitting the rows in a large csv file by its value on specified column. Each partition will form up a mini-batch in the parallel processing procedure.\r\n",
        "\r\n",
        "The outline of this notebook is as follows:\r\n",
        "\r\n",
        "- Create a tabular dataset partitioned by value on specified column.\r\n",
        "- Do ML training of forecast model per each partition\r\n",
        "- Do batch inference on the dataset with each mini-batch corresponds to one partition.\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.workspace import Workspace\r\n",
        "\r\n",
        "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
        "from azureml.core import Workspace\r\n",
        "\r\n",
        "subscription_id = '43464086-3df3-46f2-8194-66a0f80095a6'\r\n",
        "# Azure Machine Learning resource group NOT the managed resource group\r\n",
        "resource_group = 'ml-resource-group' \r\n",
        "\r\n",
        "#Azure Machine Learning workspace name, NOT Azure Databricks workspace\r\n",
        "workspace_name = 'ml-workspace'  \r\n",
        "\r\n",
        "\r\n",
        "#auth = InteractiveLoginAuthentication(tenant_id =tenant_id)\r\n",
        "# Instantiate Azure Machine Learning workspace\r\n",
        "ws = Workspace.get(name=workspace_name,\r\n",
        "                   subscription_id=subscription_id,\r\n",
        "                   resource_group=resource_group)\r\n",
        "\r\n",
        "print('Workspace name: ' + ws.name, \r\n",
        "      'Azure region: ' + ws.location, \r\n",
        "      'Subscription id: ' + ws.subscription_id, \r\n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')\r\n",
        "\r\n",
        "datastore = ws.get_default_datastore()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Workspace name: ml-workspace\nAzure region: westeurope\nSubscription id: 43464086-3df3-46f2-8194-66a0f80095a6\nResource group: ml-resource-group\n"
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.authentication import MsiAuthentication,ServicePrincipalAuthentication,TokenAuthentication, Audience\r\n",
        "msi_auth = MsiAuthentication()\r\n",
        "msi_auth\r\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "<azureml.core.authentication.MsiAuthentication at 0x7f16f62f9a60>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\r\n",
        "print(azureml.core.VERSION)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1.30.0\n"
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download OJ sales data from opendataset url"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        " #! pip install azureml-opendatasets --user"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "from azureml.opendatasets import OjSalesSimulated\r\n",
        "dataset_maxfiles = 10 # Set to 11973 or 0 to get all the files\r\n",
        "\r\n",
        "# Pull all of the data\r\n",
        "oj_sales_files = OjSalesSimulated.get_file_dataset()\r\n",
        "\r\n",
        "# Pull only the first `dataset_maxfiles` files\r\n",
        "if dataset_maxfiles:\r\n",
        "    oj_sales_files = oj_sales_files.take(dataset_maxfiles)\r\n",
        "\r\n",
        "# Create a folder to download\r\n",
        "download_path = 'data/oj_sales_data' \r\n",
        "os.makedirs(download_path, exist_ok=True)\r\n",
        "\r\n",
        "# Download the data\r\n",
        "oj_sales_files.download(download_path, overwrite=True,)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "['/mnt/batch/tasks/shared/LS_root/mounts/clusters/kchatziprmou2/code/Users/kchatziprmou/csa-misc-utils/sa-dsml-many-models/code/aml_prs/data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1000_dominicks.csv',\n '/mnt/batch/tasks/shared/LS_root/mounts/clusters/kchatziprmou2/code/Users/kchatziprmou/csa-misc-utils/sa-dsml-many-models/code/aml_prs/data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1000_minute.maid.csv',\n '/mnt/batch/tasks/shared/LS_root/mounts/clusters/kchatziprmou2/code/Users/kchatziprmou/csa-misc-utils/sa-dsml-many-models/code/aml_prs/data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1000_tropicana.csv',\n '/mnt/batch/tasks/shared/LS_root/mounts/clusters/kchatziprmou2/code/Users/kchatziprmou/csa-misc-utils/sa-dsml-many-models/code/aml_prs/data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1001_dominicks.csv',\n '/mnt/batch/tasks/shared/LS_root/mounts/clusters/kchatziprmou2/code/Users/kchatziprmou/csa-misc-utils/sa-dsml-many-models/code/aml_prs/data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1001_minute.maid.csv',\n '/mnt/batch/tasks/shared/LS_root/mounts/clusters/kchatziprmou2/code/Users/kchatziprmou/csa-misc-utils/sa-dsml-many-models/code/aml_prs/data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1001_tropicana.csv',\n '/mnt/batch/tasks/shared/LS_root/mounts/clusters/kchatziprmou2/code/Users/kchatziprmou/csa-misc-utils/sa-dsml-many-models/code/aml_prs/data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1002_dominicks.csv',\n '/mnt/batch/tasks/shared/LS_root/mounts/clusters/kchatziprmou2/code/Users/kchatziprmou/csa-misc-utils/sa-dsml-many-models/code/aml_prs/data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1002_minute.maid.csv',\n '/mnt/batch/tasks/shared/LS_root/mounts/clusters/kchatziprmou2/code/Users/kchatziprmou/csa-misc-utils/sa-dsml-many-models/code/aml_prs/data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1002_tropicana.csv',\n '/mnt/batch/tasks/shared/LS_root/mounts/clusters/kchatziprmou2/code/Users/kchatziprmou/csa-misc-utils/sa-dsml-many-models/code/aml_prs/data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1003_dominicks.csv']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload OJ sales data to datastore"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "target_path = 'oj_sales_data'\r\n",
        "\r\n",
        "datastore.upload(src_dir = download_path,\r\n",
        "                target_path = target_path,\r\n",
        "                overwrite = True, \r\n",
        "                show_progress = True)\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Uploading an estimated of 12 files\nUploading data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/.amlignore\nUploaded data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/.amlignore, 1 files out of an estimated total of 12\nUploading data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/.amlignore.amltmp\nUploaded data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/.amlignore.amltmp, 2 files out of an estimated total of 12\nUploading data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1000_dominicks.csv\nUploaded data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1000_dominicks.csv, 3 files out of an estimated total of 12\nUploading data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1000_minute.maid.csv\nUploaded data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1000_minute.maid.csv, 4 files out of an estimated total of 12\nUploading data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1000_tropicana.csv\nUploaded data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1000_tropicana.csv, 5 files out of an estimated total of 12\nUploading data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1001_dominicks.csv\nUploaded data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1001_dominicks.csv, 6 files out of an estimated total of 12\nUploading data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1001_minute.maid.csv\nUploaded data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1001_minute.maid.csv, 7 files out of an estimated total of 12\nUploading data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1001_tropicana.csv\nUploaded data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1001_tropicana.csv, 8 files out of an estimated total of 12\nUploading data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1002_dominicks.csv\nUploaded data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1002_dominicks.csv, 9 files out of an estimated total of 12\nUploading data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1002_minute.maid.csv\nUploaded data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1002_minute.maid.csv, 10 files out of an estimated total of 12\nUploading data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1002_tropicana.csv\nUploaded data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1002_tropicana.csv, 11 files out of an estimated total of 12\nUploading data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1003_dominicks.csv\nUploaded data/oj_sales_data/https%3A/%2Fazureopendatastorage.azurefd.net/ojsales-simulatedcontainer/oj_sales_data/Store1003_dominicks.csv, 12 files out of an estimated total of 12\nUploaded 12 files\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "$AZUREML_DATAREFERENCE_01dd060ea18840b2845e0653d657dac8"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create tabular dataset\n",
        "Create normal tabular dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\r\n",
        "\r\n",
        "dataset = Dataset.Tabular.from_delimited_files(path=(datastore, 'oj_sales_data/*/*/*/*/*.csv'))\r\n",
        "print(dataset.to_pandas_dataframe())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "     WeekStarting  Store      Brand  Quantity  Advert  Price   Revenue\n0      1990-06-14   1000  dominicks     12003       1   2.59  31087.77\n1      1990-06-21   1000  dominicks     10239       1   2.39  24471.21\n2      1990-06-28   1000  dominicks     17917       1   2.48  44434.16\n3      1990-07-05   1000  dominicks     14218       1   2.33  33127.94\n4      1990-07-12   1000  dominicks     15925       1   2.01  32009.25\n...           ...    ...        ...       ...     ...    ...       ...\n1205   1992-09-03   1003  dominicks     10302       1   1.94  19985.88\n1206   1992-09-10   1003  dominicks     13502       1   2.16  29164.32\n1207   1992-09-17   1003  dominicks     19644       1   2.67  52449.48\n1208   1992-09-24   1003  dominicks     13860       1   2.29  31739.40\n1209   1992-10-01   1003  dominicks     11040       1   1.99  21969.60\n\n[1210 rows x 7 columns]\n"
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partition the tabular dataset\n",
        "Partition the dataset by column 'store' and 'brand'. You can get a partition of data by specifying the value of one or more partition keys. E.g., by specifying `store=1000 and brand='tropicana'`, you can get all the rows that matches this condition in the dataset."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "partitioned_dataset = dataset.partition_by(partition_keys=['Store', 'Brand'], target=(datastore, \"partition_by_key_res\"), name=\"partitioned_oj_data\")\r\n",
        "partitioned_dataset.partition_keys"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Method partition_by: This is an experimental method, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Validating arguments.\nArguments validated.\nUploading file to /partition_by_key_res/b1b7783e-5690-4349-955b-6e6b8e530511/\nSuccessfully uploaded file to datastore.\nCreating a new dataset.\nSuccessfully created a new dataset.\nregistering a new dataset.\nSuccessfully created and registered a new dataset.\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "['Store', 'Brand']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create or Attach existing compute resource"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "from azureml.core.compute import AmlCompute, ComputeTarget\r\n",
        "\r\n",
        "compute_target = ComputeTarget(workspace=ws, name=\"cpu-cluster\")\r\n",
        "\r\n",
        "# choose a name for your cluster\r\n",
        "#compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster\")\r\n",
        "#compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\r\n",
        "#compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 2)\r\n",
        "\r\n",
        "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\r\n",
        "#vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\r\n",
        "\r\n",
        "\r\n",
        "#if compute_name in ws.compute_targets:\r\n",
        "   # compute_target = ws.compute_targets[compute_name]\r\n",
        "    #if compute_target and type(compute_target) is AmlCompute:\r\n",
        "       # print('found compute target. just use it. ' + compute_name)\r\n",
        "#else:\r\n",
        "   # print('creating a new compute target...')\r\n",
        "    #provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\r\n",
        "                                                               # min_nodes = compute_min_nodes, \r\n",
        "                                                               # max_nodes = compute_max_nodes)\r\n",
        "\r\n",
        "    # create the cluster\r\n",
        "   # compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\r\n",
        "    \r\n",
        "    # can poll for a minimum number of nodes and for a specific timeout. \r\n",
        "    # if no min node count is provided it will use the scale settings for the cluster\r\n",
        "    #compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\r\n",
        "    \r\n",
        "     # For a more detailed view of current AmlCompute status, use get_status()\r\n",
        "    #print(compute_target.get_status().serialize())"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intermediate/Output Data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import Pipeline, PipelineData\n",
        "\n",
        "output_dir = PipelineData(name=\"inferences\", datastore=datastore)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "scripts_folder = \"..\\..\\code\"\r\n",
        "inference_script_file = \"aml_prs\\prediction.py\"\r\n",
        "train_script_file = \"aml_prs\\model_train.py\""
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build and run the model training  pipeline\r\n",
        "### Specify the environment to run the script\r\n",
        "You would need to specify the required private azureml packages in dependencies. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\r\n",
        "from azureml.core.runconfig import CondaDependencies, DEFAULT_CPU_IMAGE\r\n",
        "\r\n",
        "batch_conda_deps = CondaDependencies.create(pip_packages=['sklearn', 'pandas', 'joblib', 'azureml-defaults', 'azureml-core', 'azureml-dataprep[fuse]'])\r\n",
        "batch_env = Environment(name=\"many_models_environment\")\r\n",
        "batch_env.python.conda_dependencies = batch_conda_deps\r\n",
        "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training \r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the configuration to wrap the training script\r\n",
        "The parameter `partition_keys` is a list containing a subset of the dataset partition keys, specifying how is the input dataset partitioned. Each and every possible combination of values of partition_keys will form up a mini-batch. E.g., by specifying `partition_keys=['store', 'brand']` will result in mini-batches like `store=1000 && brand=tropicana`, `store=1000 && brand=dominicks`, `store=1001 && brand=dominicks`, ..."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\r\n",
        "\r\n",
        "# In a real-world scenario, you'll want to shape your process per node and nodes to fit your problem domain.\r\n",
        "parallel_run_train_config = ParallelRunConfig(\r\n",
        "    source_directory=scripts_folder,\r\n",
        "    entry_script=train_script_file,  # the user script to run against each input\r\n",
        "    partition_keys=['Store', 'Brand'],\r\n",
        "    error_threshold=-1,\r\n",
        "    output_action='append_row',\r\n",
        "    append_row_file_name=\"training_output.txt\",\r\n",
        "    environment=batch_env,\r\n",
        "    compute_target=compute_target, \r\n",
        "    node_count=2,\r\n",
        "    run_invocation_timeout=600\r\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Parameter partition_keys: This is an experimental parameter, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
        }
      ],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_run_train_step = ParallelRunStep(\r\n",
        "    name='train',\r\n",
        "    inputs=[partitioned_dataset.as_named_input(\"partitioned_tabular_input\")],\r\n",
        "    output=output_dir,\r\n",
        "    parallel_run_config=parallel_run_train_config,\r\n",
        "    allow_reuse=False\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 40,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\r\n",
        "from azureml.pipeline.core import Pipeline\r\n",
        "\r\n",
        "pipeline = Pipeline(workspace=ws, steps=[parallel_run_train_step])\r\n",
        "\r\n",
        "pipeline_run = Experiment(ws, 'Many_Model_Forecast').submit(pipeline)"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View the training results\r\n",
        "In the model_train.py file you can see that the ResultList with training metrics gets returned. These are written to the DataStore specified in the PipelineData object as the output data, which in this case is called inferences. This containers the outputs from all of the worker nodes used in the compute cluster. You can download this data to view the results ... below just filters to the first 10 rows"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\r\n",
        "import tempfile\r\n",
        "\r\n",
        "batch_run = pipeline_run.find_step_run(parallel_run_train_step.name)[0]\r\n",
        "batch_output = batch_run.get_output_data(output_dir.name)\r\n",
        "\r\n",
        "target_dir = tempfile.mkdtemp()\r\n",
        "batch_output.download(local_path=target_dir)\r\n",
        "result_file = os.path.join(target_dir, batch_output.path_on_datastore, parallel_run_train_config.append_row_file_name)\r\n",
        "\r\n",
        "df = pd.read_csv(result_file, delimiter=\" \", header=None)\r\n",
        "\r\n",
        "df.columns = [ \"Store\", \"Brand\", \"mse\", \"mape\", \"rmse\", \"model_name\"]\r\n",
        "print(\"Train result has \", df.shape[0], \" rows\")\r\n",
        "df.head(10)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Train result has  10  rows\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "   Store        Brand           mse       mape         rmse  \\\n0   1000  minute.maid  1.022011e+07  18.172361  3196.891234   \n1   1001    tropicana  8.552595e+06  19.463062  2924.481928   \n2   1000    dominicks  1.080181e+07  23.172431  3286.611380   \n3   1001    dominicks  8.920132e+06  20.350161  2986.658951   \n4   1001  minute.maid  1.556051e+07  26.075884  3944.680670   \n5   1003    dominicks  1.278600e+07  23.512086  3575.751535   \n6   1002    tropicana  1.069882e+07  20.099309  3270.904584   \n7   1000    tropicana  9.844161e+06  19.846187  3137.540593   \n8   1002    dominicks  9.404042e+06  23.352023  3066.601004   \n9   1002  minute.maid  6.130244e+06  17.805611  2475.932918   \n\n             model_name  \n0  prs_1000_minute.maid  \n1    prs_1001_tropicana  \n2    prs_1000_dominicks  \n3    prs_1001_dominicks  \n4  prs_1001_minute.maid  \n5    prs_1003_dominicks  \n6    prs_1002_tropicana  \n7    prs_1000_tropicana  \n8    prs_1002_dominicks  \n9  prs_1002_minute.maid  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Store</th>\n      <th>Brand</th>\n      <th>mse</th>\n      <th>mape</th>\n      <th>rmse</th>\n      <th>model_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000</td>\n      <td>minute.maid</td>\n      <td>1.022011e+07</td>\n      <td>18.172361</td>\n      <td>3196.891234</td>\n      <td>prs_1000_minute.maid</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1001</td>\n      <td>tropicana</td>\n      <td>8.552595e+06</td>\n      <td>19.463062</td>\n      <td>2924.481928</td>\n      <td>prs_1001_tropicana</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000</td>\n      <td>dominicks</td>\n      <td>1.080181e+07</td>\n      <td>23.172431</td>\n      <td>3286.611380</td>\n      <td>prs_1000_dominicks</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1001</td>\n      <td>dominicks</td>\n      <td>8.920132e+06</td>\n      <td>20.350161</td>\n      <td>2986.658951</td>\n      <td>prs_1001_dominicks</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1001</td>\n      <td>minute.maid</td>\n      <td>1.556051e+07</td>\n      <td>26.075884</td>\n      <td>3944.680670</td>\n      <td>prs_1001_minute.maid</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1003</td>\n      <td>dominicks</td>\n      <td>1.278600e+07</td>\n      <td>23.512086</td>\n      <td>3575.751535</td>\n      <td>prs_1003_dominicks</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1002</td>\n      <td>tropicana</td>\n      <td>1.069882e+07</td>\n      <td>20.099309</td>\n      <td>3270.904584</td>\n      <td>prs_1002_tropicana</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1000</td>\n      <td>tropicana</td>\n      <td>9.844161e+06</td>\n      <td>19.846187</td>\n      <td>3137.540593</td>\n      <td>prs_1000_tropicana</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1002</td>\n      <td>dominicks</td>\n      <td>9.404042e+06</td>\n      <td>23.352023</td>\n      <td>3066.601004</td>\n      <td>prs_1002_dominicks</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1002</td>\n      <td>minute.maid</td>\n      <td>6.130244e+06</td>\n      <td>17.805611</td>\n      <td>2475.932918</td>\n      <td>prs_1002_minute.maid</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the configuration to wrap the inference script\n",
        "The parameter `partition_keys` is a list containing a subset of the dataset partition keys, specifying how is the input dataset partitioned. Each and every possible combination of values of partition_keys will form up a mini-batch. E.g., by specifying `partition_keys=['store', 'brand']` will result in mini-batches like `store=1000 && brand=tropicana`, `store=1000 && brand=dominicks`, `store=1001 && brand=dominicks`, ..."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\r\n",
        "\r\n",
        "# In a real-world scenario, you'll want to shape your process per node and nodes to fit your problem domain.\r\n",
        "parallel_run_inference_config = ParallelRunConfig(\r\n",
        "    source_directory=scripts_folder,\r\n",
        "    entry_script=inference_script_file,  # the user script to run against each input\r\n",
        "    partition_keys=['Store', 'Brand'],\r\n",
        "    error_threshold= -1,\r\n",
        "    output_action='append_row',\r\n",
        "    append_row_file_name=\"prediction_output.txt\",\r\n",
        "    environment=batch_env,\r\n",
        "    compute_target=compute_target, \r\n",
        "    node_count=2,\r\n",
        "    run_invocation_timeout=600\r\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "WARNING - Parameter partition_keys: This is an experimental parameter, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
        }
      ],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the pipeline step"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_run_inference_step = ParallelRunStep(\r\n",
        "    name='forecast',\r\n",
        "    inputs=[partitioned_dataset.as_named_input(\"partitioned_tabular_input\")],\r\n",
        "    output=output_dir,\r\n",
        "    parallel_run_config=parallel_run_inference_config,\r\n",
        "    allow_reuse=False\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the pipeline"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\r\n",
        "from azureml.pipeline.core import Pipeline\r\n",
        "\r\n",
        "pipeline = Pipeline(workspace=ws, steps=[parallel_run_inference_step])\r\n",
        "\r\n",
        "pipeline_run = Experiment(ws, 'Many_Model_Forecast').submit(pipeline)"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View the prediction results\r\n",
        "In the prediction.py file you can see that the ResultList with prediction result gets returned. These are written to the DataStore specified in the PipelineData object as the output data, which in this case is called inferences. This containers the outputs from all of the worker nodes used in the compute cluster. You can download this data to view the results ... below just filters to the first 10 rows"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\r\n",
        "import tempfile\r\n",
        "\r\n",
        "batch_run = pipeline_run.find_step_run(parallel_run_inference_step.name)[0]\r\n",
        "batch_output = batch_run.get_output_data(output_dir.name)\r\n",
        "\r\n",
        "target_dir = tempfile.mkdtemp()\r\n",
        "batch_output.download(local_path=target_dir)\r\n",
        "result_file = os.path.join(target_dir, batch_output.path_on_datastore, parallel_run_config.append_row_file_name)\r\n",
        "\r\n",
        "df = pd.read_csv(result_file, delimiter=\" \", header=None)\r\n",
        "df.columns = [\"WeekStarting\", \"Prediction\", \"Store\", \"Brand\"]\r\n",
        "print(\"Prediction has \", df.shape[0], \" rows\")\r\n",
        "df.head(10)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Prediction has  1210  rows\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "  WeekStarting    Prediction  Store      Brand\n0   1990-06-14           NaN   1001  dominicks\n1   1990-06-21           NaN   1001  dominicks\n2   1990-06-28           NaN   1001  dominicks\n3   1990-07-05           NaN   1001  dominicks\n4   1990-07-12  13554.387879   1001  dominicks\n5   1990-07-19  15375.931186   1001  dominicks\n6   1990-07-26  15243.409687   1001  dominicks\n7   1990-08-02  13649.643619   1001  dominicks\n8   1990-08-09  14017.612778   1001  dominicks\n9   1990-08-16  15976.467391   1001  dominicks",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>WeekStarting</th>\n      <th>Prediction</th>\n      <th>Store</th>\n      <th>Brand</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1990-06-14</td>\n      <td>NaN</td>\n      <td>1001</td>\n      <td>dominicks</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1990-06-21</td>\n      <td>NaN</td>\n      <td>1001</td>\n      <td>dominicks</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1990-06-28</td>\n      <td>NaN</td>\n      <td>1001</td>\n      <td>dominicks</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1990-07-05</td>\n      <td>NaN</td>\n      <td>1001</td>\n      <td>dominicks</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1990-07-12</td>\n      <td>13554.387879</td>\n      <td>1001</td>\n      <td>dominicks</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1990-07-19</td>\n      <td>15375.931186</td>\n      <td>1001</td>\n      <td>dominicks</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1990-07-26</td>\n      <td>15243.409687</td>\n      <td>1001</td>\n      <td>dominicks</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1990-08-02</td>\n      <td>13649.643619</td>\n      <td>1001</td>\n      <td>dominicks</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1990-08-09</td>\n      <td>14017.612778</td>\n      <td>1001</td>\n      <td>dominicks</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1990-08-16</td>\n      <td>15976.467391</td>\n      <td>1001</td>\n      <td>dominicks</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {}
    }
  ],
  "metadata": {
    "index_order": 1,
    "exclude_from_index": false,
    "deployment": [
      "None"
    ],
    "authors": [
      {
        "name": "pansav"
      },
      {
        "name": "tracych"
      },
      {
        "name": "migu"
      }
    ],
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "AML Compute"
    ],
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "interpreter": {
      "hash": "f7f364c9551711cd4699acda32e0312c3edab483ae246bf330de758088cecccb"
    },
    "datasets": [
      "OJ Sales Data"
    ],
    "category": "Other notebooks",
    "framework": [
      "None"
    ],
    "friendly_name": "Batch inferencing OJ Sales Data partitioned by column using ParallelRunStep",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}